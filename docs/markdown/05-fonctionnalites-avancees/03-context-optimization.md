<!-- .slide: class="transition" -->

# Optimisation du Contexte

##==##

<!-- .slide -->

# Le Challenge du Contexte

## Pourquoi optimiser ?

<br>

Dans une conversation longue, l'historique (contexte) s'accumule rapidement.

### Probl√®mes majeurs :
1. **Co√ªts Exponentiels** üí∏ : Vous repayez pour relire tout l'historique √† chaque nouvelle question.
2. **Latence** ‚è±Ô∏è : Le "Time to First Token" augmente avec la taille du prompt.
3. **Fenetre Limit√©e** ü™ü : M√™me avec 1M/2M tokens, on finit par atteindre la limite ou diluer l'attention du mod√®le ("Lost in the Middle").

<br>

### Solutions ADK :
- **Caching** : Ne pas re-uploader ce qui ne change pas.
- **Compression** : R√©sumer ce qui est vieux.

Notes:
L'optimisation du contexte est critique pour passer du prototype (chat court) √† la production (assistants de longue dur√©e).

##==##

<!-- .slide: class="with-code max-height" -->

# Context Caching

## R√©utiliser le contexte statique

Id√©al pour les gros documents ou les instructions syst√®me complexes qui ne changent pas.

```python
from google.adk import Agent
from google.adk.apps.app import App
from google.adk.agents.context_cache_config import ContextCacheConfig

root_agent = Agent(
  # configure an agent using Gemini 2.5 or higher
)
app = App(
    name='my-caching-agent-app',
    root_agent=root_agent,
    context_cache_config=ContextCacheConfig(
        min_tokens=2048,    # Nombre minimum de tokens pour activer le cache
        ttl_seconds=600,    # Cache valide pendant 10 minutes
        cache_intervals=5,  # Met √† jour le cache tous les 5 appels
    ),
)
```

Le mod√®le charge le contexte une fois, et les appels suivants sont beaucoup plus rapides et moins chers (tarif "cached input").

<!-- .element: class="admonition note" -->

Notes:
Gemini offre du "Context Caching" explicite. ADK le g√®re pour vous via cette config.

##==##

<!-- .slide: class="with-code max-height" -->

# Context Compression

Pour g√©rer une conversation "infinie", on ne peut pas tout garder. La compression r√©sume le pass√©.

## Workflow de Compression

![](./assets/images/context-compaction.png)

- **√âv√©nement 3 termin√©** : Les 3 premiers √©v√©nements sont compress√©s en un r√©sum√©.
- **√âv√©nement 6 termin√©** : Les √©v√©nements 3 √† 6 sont compress√©s, avec un chevauchement d'un √©v√©nement pr√©c√©dent.

Notes:
C'est transparent pour l'utilisateur. Le mod√®le a "m√©moire" des faits anciens via le r√©sum√©, mais travaille sur un contexte court.

##==##
<!-- .slide: class="with-code max-height" -->
# Impl√©mentation de la Compression

```python
from google.adk.apps.app import App, EventsCompactionConfig
from google.adk.apps.llm_event_summarizer import LlmEventSummarizer
from google.adk.models import Gemini

# Define the AI model to be used for summarization:
summarization_llm = Gemini(model="gemini-2.5-flash")

# Create the summarizer with the custom model:
my_summarizer = LlmEventSummarizer(llm=summarization_llm)

# Configure the App with the custom summarizer and compaction settings:
app = App(
    name='my-agent',
    root_agent=root_agent,
    events_compaction_config=EventsCompactionConfig(
        compaction_interval=3,
        overlap_size=1,
        summarizer=my_summarizer,
    ),
)
```
